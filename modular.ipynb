{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4827fd4d-7f8e-42ed-9d63-248154f027fa",
   "metadata": {},
   "source": [
    "# Creating a Modular Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "994d8dc5-76ec-43dc-82a2-ba9559319f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a directory to store the scripts\n",
    "import os\n",
    "os.makedirs('module', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f006fe30-9844-45af-9145-408a061f4485",
   "metadata": {},
   "source": [
    "## Create Dataset - data_setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "830d3cd5-b2fa-43db-bf95-3c82b23ad71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing module/data_setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile module/data_setup.py\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "class Dataset:\n",
    "    \"\"\"\n",
    "    Creates a tensorflow dataset for video files.\n",
    "    The data needs to been in a Imagenet directory structure.\n",
    "    After processing the data, two tf.data train and test will be returned.\n",
    "    \n",
    "    Parameters: \n",
    "        data_path: A string of the parent directory for all the data.\n",
    "        class_list: A list containing classes names that will be needed in the dataset.\n",
    "        seq_len(default=20): A integer for selecting total frames from the video.\n",
    "        frame_size(default=128): A integer for resizing height and width of the frames.\n",
    "        batch_size(default=32): A integer for selecting the size of a batch.\n",
    "        seed(default=42): A integer for controlling the randomness of random numbers generator.\n",
    "    \n",
    "    Returns:\n",
    "        train_ds, test_ds: A tuple of training and testing dataset pipeline. \n",
    "    \"\"\"\n",
    "    def __init__(self, data_path, seq_len = 20, frame_size = 128, batch_size = 32, seed = 42, class_list=None):\n",
    "        self.data_path = data_path\n",
    "        self.seq_len = seq_len\n",
    "        self.frame_size = frame_size\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = seed\n",
    "        \n",
    "        # Handling class_list\n",
    "        if class_list==None:\n",
    "            self.classes = sorted(os.listdir(self.data_path))\n",
    "        else:\n",
    "            self.classes = sorted(class_list)\n",
    "    \n",
    "    def frames_extraction(self, video_file_path):\n",
    "        frames_list = []\n",
    "        # Reading the video file and counting the frames\n",
    "        video_reader = cv2.VideoCapture(video_file_path)\n",
    "        video_frame_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        # Selecting the frames at certain interval and applying the transformation\n",
    "        skip_frames = max(int(video_frame_count/self.seq_len), 1)\n",
    "        for i in range(self.seq_len):\n",
    "            video_reader.set(cv2.CAP_PROP_POS_FRAMES, i * skip_frames)\n",
    "            success, frame = video_reader.read()\n",
    "            if not success:\n",
    "                break\n",
    "            resize_frame = cv2.resize(frame, (self.frame_size, self.frame_size))\n",
    "            norm_frame = resize_frame/255.\n",
    "            frames_list.append(norm_frame.astype('float32'))\n",
    "        video_reader.release()\n",
    "        return frames_list\n",
    "    \n",
    "    def create_dataset(self):\n",
    "        features = []\n",
    "        labels = []\n",
    "        video_files_path = []\n",
    "        \n",
    "        # Going through all the data in the class list\n",
    "        print(f'[INFO] Extracting data from {len(self.classes)} classes...')\n",
    "        for i, class_name in enumerate(self.classes):\n",
    "            print(f'[INFO] Extracting all the data in the class: {class_name}')\n",
    "\n",
    "            # Getting the list of all the video files and the path to the video\n",
    "            files_list = os.listdir(os.path.join(self.data_path, class_name))\n",
    "            for file_name in files_list:\n",
    "                video_file_path = os.path.join(self.data_path, class_name, file_name)\n",
    "\n",
    "                # Extracting frames using the function and verifying the total frames\n",
    "                frames_list = self.frames_extraction(video_file_path=video_file_path)\n",
    "                if len(frames_list) == self.seq_len:\n",
    "\n",
    "                    # Appending the data in a list\n",
    "                    features.append(frames_list)\n",
    "                    labels.append(i)\n",
    "                    video_files_path.append(video_file_path)\n",
    "        \n",
    "        # Converting the list to array\n",
    "        features = np.asarray(features)\n",
    "        labels = np.asarray(labels)\n",
    "        \n",
    "        # Hot encoding the labels\n",
    "        labels = to_categorical(labels)\n",
    "        print('[INFO] Datset is been created')\n",
    "        return features, labels, video_files_path\n",
    "    \n",
    "    def split_dataset(self):\n",
    "        features, labels, video_files_path = self.create_dataset()\n",
    "        train_features, test_features, train_labels, test_labels = train_test_split(features, \n",
    "                                                                                    labels, \n",
    "                                                                                    test_size=0.25, \n",
    "                                                                                    shuffle=True, \n",
    "                                                                                    random_state=self.seed)\n",
    "        print('[INFO] Dataset is been splitted into train and test set.')\n",
    "        return train_features, test_features, train_labels, test_labels\n",
    "    \n",
    "    def dataset_pipeline(self):\n",
    "        train_features, test_features, train_labels, test_labels = self.split_dataset()\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((train_features,\n",
    "                                                       train_labels)).shuffle(10000, self.seed).batch(self.batch_size, True).prefetch(tf.data.AUTOTUNE)\n",
    "        test_ds = tf.data.Dataset.from_tensor_slices((test_features,\n",
    "                                                      test_labels)).batch(self.batch_size, True).prefetch(tf.data.AUTOTUNE)\n",
    "        print('[INFO] Dataset pipeline is been created')\n",
    "        return train_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9ab1f19-c58d-4830-b783-05fc21db07c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Extracting data from 5 classes...\n",
      "[INFO] Extracting all the data in the class: Biking\n",
      "[INFO] Extracting all the data in the class: Diving\n",
      "[INFO] Extracting all the data in the class: GolfSwing\n",
      "[INFO] Extracting all the data in the class: Punch\n",
      "[INFO] Extracting all the data in the class: Rowing\n",
      "[INFO] Datset is been created\n",
      "[INFO] Dataset is been splitted into train and test set.\n",
      "[INFO] Dataset pipeline is been created\n"
     ]
    }
   ],
   "source": [
    "from module.data_setup import Dataset\n",
    "dataset = Dataset(data_path='data/UCF50', class_list=['Biking', 'Diving', 'GolfSwing', 'Punch', 'Rowing'])\n",
    "train_ds, test_ds = dataset.dataset_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40e686a2-f1dd-428e-8d89-6b7fa827e5c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c39440-16b8-438f-aa7a-374100bfb6c8",
   "metadata": {},
   "source": [
    "## Create Model - model_builder.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "905981b8-8957-4105-80da-ea6c764fd7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing module/model_builder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile module/model_builder.py\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import ConvLSTM2D, Conv2D, MaxPooling3D, MaxPooling2D, LSTM, TimeDistributed, Dropout, Flatten, Dense\n",
    "\n",
    "class CreateConvlstmModel(Model):\n",
    "    \"\"\"\n",
    "    Constructs and Initiates a ConvLSTM model for video classification.\n",
    "\n",
    "    Parameters: \n",
    "        input_shape: tuple, Input shape of the array that is feeded in the model.\n",
    "                     Format of the input_shape should be (timesteps, height, width, channels)\n",
    "        num_classes: int, Total number of classes that model needs to predict.\n",
    "\n",
    "    Returns: Fully Constructed ConvLSTM Model.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape: tuple, num_classes: int):\n",
    "        super(CreateConvlstmModel, self).__init__()\n",
    "        self.input_block1 = Sequential([\n",
    "            ConvLSTM2D(4, 3, activation='tanh', data_format='channels_last', recurrent_dropout=0.2, return_sequences=True, input_shape=input_shape),\n",
    "            MaxPooling3D((1, 2, 2), padding='same', data_format='channels_last'),\n",
    "            TimeDistributed(Dropout(0.2))\n",
    "        ])\n",
    "        self.block2 = Sequential([\n",
    "            ConvLSTM2D(8, 3, activation='tanh', data_format='channels_last', recurrent_dropout=0.2, return_sequences=True),\n",
    "            MaxPooling3D((1, 2, 2), padding='same', data_format='channels_last'),\n",
    "            TimeDistributed(Dropout(0.2))\n",
    "        ])\n",
    "        self.block3 = Sequential([\n",
    "            ConvLSTM2D(12, 3, activation='tanh', data_format='channels_last', recurrent_dropout=0.2, return_sequences=True),\n",
    "            MaxPooling3D((1, 2, 2), padding='same', data_format='channels_last'),\n",
    "            TimeDistributed(Dropout(0.2))\n",
    "        ])\n",
    "        self.block4 = Sequential([\n",
    "            ConvLSTM2D(16, 3, activation='tanh', data_format='channels_last', recurrent_dropout=0.2, return_sequences=True),\n",
    "            MaxPooling3D((1, 2, 2), padding='same', data_format='channels_last')\n",
    "        ])\n",
    "        self.classifier_block = Sequential([\n",
    "            Flatten(),\n",
    "            Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.input_block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        return self.classifier_block(x)\n",
    "\n",
    "class CreateLRCNModel(Model):\n",
    "    \"\"\"\n",
    "    Constructs and Initiates a LRCN model for video classification.\n",
    "\n",
    "    Parameters: \n",
    "        input_shape: tuple, Input shape of the array that is feeded in the model.\n",
    "                     Format of the input_shape should be (timesteps, height, width, channels)\n",
    "        num_classes: int, Total number of classes that model needs to predict.\n",
    "\n",
    "    Returns: Fully Constructed LRCN Model.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape: tuple, num_classes: int):\n",
    "        super(CreateLRCNModel, self).__init__()\n",
    "        self.input_block1 = Sequential([\n",
    "            TimeDistributed(Conv2D(16, 3, padding='same', activation='relu'), input_shape=input_shape),\n",
    "            TimeDistributed(MaxPooling2D(4)),\n",
    "            TimeDistributed(Dropout(0.25))\n",
    "        ])\n",
    "        self.block2 = Sequential([\n",
    "            TimeDistributed(Conv2D(32, 3, padding='same', activation='relu')),\n",
    "            TimeDistributed(MaxPooling2D(4)),\n",
    "            TimeDistributed(Dropout(0.25))\n",
    "        ])\n",
    "        self.block3 = Sequential([\n",
    "            TimeDistributed(Conv2D(64, 3, padding='same', activation='relu')),\n",
    "            TimeDistributed(MaxPooling2D(2)),\n",
    "            TimeDistributed(Dropout(0.25))\n",
    "        ])\n",
    "        self.block4 = Sequential([\n",
    "            TimeDistributed(Conv2D(64, 3, padding='same', activation='relu')),\n",
    "            TimeDistributed(MaxPooling2D(2))\n",
    "        ])\n",
    "        self.classifier_block = Sequential([\n",
    "            TimeDistributed(Flatten()),\n",
    "            LSTM(32),\n",
    "            Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.input_block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        return self.classifier_block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4e0debb-2a44-45c1-926f-bf56eec54759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"create_lrcn_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential (Sequential)     (None, 20, 32, 32, 16)    448       \n",
      "                                                                 \n",
      " sequential_1 (Sequential)   (32, 20, 8, 8, 32)        4640      \n",
      "                                                                 \n",
      " sequential_2 (Sequential)   (32, 20, 4, 4, 64)        18496     \n",
      "                                                                 \n",
      " sequential_3 (Sequential)   (32, 20, 2, 2, 64)        36928     \n",
      "                                                                 \n",
      " sequential_4 (Sequential)   (32, 2)                   37058     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 97,570\n",
      "Trainable params: 97,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from module.model_builder import CreateConvlstmModel, CreateLRCNModel\n",
    "model = CreateLRCNModel((20, 128, 128, 3), 2)\n",
    "model.build((32, 20, 128, 128, 3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3ee35c-d1e7-49ed-8f31-4462d493ccbd",
   "metadata": {},
   "source": [
    "## Create utilities - utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30f49f8c-f7b3-425b-b6b1-4a6b17348c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing module/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile module/utils.py\n",
    "import os\n",
    "import datetime as dt\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "\n",
    "def tensorboard_callback(dir_name: str, model_name: str, exp_name: str):\n",
    "    \"\"\"\n",
    "    Creates Tensorboard Callback\n",
    "    Parameters: \n",
    "        dir_name: A string to save tensorboard data in a directory.\n",
    "        model_name: A string for the model name.\n",
    "        exp_name: A string for the experiment name.\n",
    "    Returns: A pre-configured tensorboard callback.\n",
    "    \"\"\"\n",
    "    # Creating a tensorboard callback\n",
    "    log_dir = os.path.join(dir_name, model_name, exp_name, dt.datetime.now().strftime('%Y-%m-%d-%H:%M:%S'))\n",
    "    tensorboard_callback = TensorBoard(log_dir=log_dir)\n",
    "    print(f'[INFO] Saving Tensorboard log files to: {log_dir}')\n",
    "    return tensorboard_callback\n",
    "    \n",
    "# Creating a early stopping callback\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss',\n",
    "                                        patience=20,\n",
    "                                        mode='min',\n",
    "                                        verbose=1,\n",
    "                                        restore_best_weights=True)\n",
    "    \n",
    "# Creating a reduce learning rate callback\n",
    "reduce_lr_callback = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                       mode='min',\n",
    "                                       factor=0.2,\n",
    "                                       patience=10,\n",
    "                                       verbose=1,\n",
    "                                       min_lr=1e-7)\n",
    "\n",
    "def save_model(model, dir_name: str, model_name: str, exp_name: str):\n",
    "    \"\"\"\n",
    "    A function to save a tensorflow model.\n",
    "    Parameters: \n",
    "        model: A trained model.\n",
    "        dir_name: A string to save whole model data in a directory.\n",
    "        model_name: A string for the model name.\n",
    "        exp_name: A string for the experiment name.\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(dir_name, model_name, exp_name, dt.datetime.now().strftime('%Y-%m-%d-%H:%M:%S'))\n",
    "    model.save(filepath=filepath, save_format='tf')\n",
    "    print(f'[INFO] \"{model_name}\" Model is been saved to directory: {filepath}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e38b913-d127-435a-af3d-b234b57f0eb1",
   "metadata": {},
   "source": [
    "## Train and Save the model - train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5930c830-c553-4aae-ab52-8a7e451ee7fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing module/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile module/train.py\n",
    "import os\n",
    "import argparse\n",
    "import datetime as dt\n",
    "from sys import exit\n",
    "import data_setup, model_builder, utils\n",
    "import tensorflow as tf\n",
    "\n",
    "# Creating a parser\n",
    "parser = argparse.ArgumentParser(description='Get some hyperparameters')\n",
    "\n",
    "# Getting hyperparameters\n",
    "# Model name\n",
    "parser.add_argument('--model_name',\n",
    "                    default='LRCN',\n",
    "                    choices=('ConvLSTM', 'LRCN'),\n",
    "                    type=str,\n",
    "                    help='Name of the model')\n",
    "\n",
    "# Experiment name\n",
    "parser.add_argument('--exp_name',\n",
    "                    default='experiment',\n",
    "                    type=str,\n",
    "                    help='Name of the experiment')\n",
    "\n",
    "# Number of epochs\n",
    "parser.add_argument('--num_epochs',\n",
    "                    default=100,\n",
    "                    type=int,\n",
    "                    help='The number of epochs to train the model')\n",
    "\n",
    "# batch size\n",
    "parser.add_argument('--batch_size',\n",
    "                    default=32,\n",
    "                    type=int,\n",
    "                    help='The number of sample for batch data')\n",
    "\n",
    "# Sequence length\n",
    "parser.add_argument('--seq_len',\n",
    "                    default=20,\n",
    "                    type=int,\n",
    "                    help='Total number of frames for every video')\n",
    "\n",
    "# Frame size\n",
    "parser.add_argument('--frame_size',\n",
    "                    default=128,\n",
    "                    type=int,\n",
    "                    help='Integer for resizing the frame')\n",
    "\n",
    "# learning rate\n",
    "parser.add_argument('--lr',\n",
    "                    default=0.001,\n",
    "                    type=float,\n",
    "                    help='Learning rate for optimizer')\n",
    "\n",
    "# Data directory path\n",
    "parser.add_argument('--data_dir_path',\n",
    "                    default='data/UCF50',\n",
    "                    type=str,\n",
    "                    help='A path for data directory')\n",
    "\n",
    "# Classes list\n",
    "parser.add_argument('--class_list',\n",
    "                    default=None,\n",
    "                    nargs='+',\n",
    "                    help='A list containing class names, use None for all the classes')\n",
    "\n",
    "# Number of workers\n",
    "parser.add_argument('--num_workers',\n",
    "                    default=os.cpu_count(),\n",
    "                    type=int,\n",
    "                    help='Workers you want to assign durning the model training.')\n",
    "\n",
    "# Callbacks\n",
    "parser.add_argument('--callbacks',\n",
    "                    default='True',\n",
    "                    choices=('True', 'False'),\n",
    "                    type=str,\n",
    "                    help='Select a boolean to use callbacks durning training.')\n",
    "\n",
    "# Getting the arguments from parser \n",
    "args = parser.parse_args()\n",
    "\n",
    "# Collecting the arguments\n",
    "MODEL_NAME = args.model_name\n",
    "EXP_NAME = args.exp_name\n",
    "NUM_EPOCHS = args.num_epochs\n",
    "BATCH_SIZE = args.batch_size\n",
    "SEQ_LEN = args.seq_len\n",
    "FRAME_SIZE = args.frame_size\n",
    "LR = args.lr\n",
    "DATA_PATH = args.data_dir_path\n",
    "CLASSES = args.class_list\n",
    "NUM_WORKERS = args.num_workers\n",
    "CALLBACKS = args.callbacks\n",
    "\n",
    "# Error handling\n",
    "if not CLASSES == None:\n",
    "    # Checking valid class list\n",
    "    ucf_class_list = data_setup.Dataset(data_path=DATA_PATH).classes\n",
    "    for i in CLASSES:\n",
    "        if i not in ucf_class_list:\n",
    "            print(f'[ERROR] \"{i}\" is a wrong class name.')\n",
    "            print(f'[INFO] Kindly select classes from this list: {ucf_class_list}')\n",
    "            exit()\n",
    "    \n",
    "    # Checking total classes used for training\n",
    "    if not len(CLASSES) >= 3:\n",
    "        print(f'[ERROR] The Class list \"{CLASSES}\", contains less classes than the requirement.')\n",
    "        print('[INFO] Minimum required classes in class list is 3. If you want to use the whole dataset than do not use this flag.')\n",
    "        exit()\n",
    "    \n",
    "print(f'\\n[INFO] Training a {MODEL_NAME} model for {NUM_EPOCHS} epochs with batch size {BATCH_SIZE} and a learning rate of {LR}')\n",
    "\n",
    "# Creating dataset using data_setup script\n",
    "dataset = data_setup.Dataset(data_path=DATA_PATH, \n",
    "                             seq_len=SEQ_LEN,\n",
    "                             frame_size=FRAME_SIZE,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             class_list=CLASSES)\n",
    "train_ds, test_ds = dataset.dataset_pipeline()\n",
    "\n",
    "# Creating model using the model_builder script\n",
    "# Getting number of classes\n",
    "if CLASSES == None:\n",
    "    NUM_CLASSES = 50\n",
    "else:\n",
    "    NUM_CLASSES = len(CLASSES)\n",
    "\n",
    "# Selecting a model and creating it.\n",
    "if MODEL_NAME == 'ConvLSTM':\n",
    "    model = model_builder.CreateConvlstmModel(input_shape = (SEQ_LEN, FRAME_SIZE, FRAME_SIZE, 3), \n",
    "                                              num_classes = NUM_CLASSES)\n",
    "    model.build((BATCH_SIZE, SEQ_LEN, FRAME_SIZE, FRAME_SIZE, 3))\n",
    "    print(f'[INFO] Model \"{MODEL_NAME}\" is been constructed.')\n",
    "elif MODEL_NAME == 'LRCN':\n",
    "    model = model_builder.CreateLRCNModel(input_shape = (SEQ_LEN, FRAME_SIZE, FRAME_SIZE, 3), \n",
    "                                          num_classes = NUM_CLASSES)\n",
    "    model.build((BATCH_SIZE, SEQ_LEN, FRAME_SIZE, FRAME_SIZE, 3))\n",
    "    print(f'[INFO] Model \"{MODEL_NAME}\" is been constructed.')\n",
    "    \n",
    "# Compiling the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Setting up callbacks\n",
    "if CALLBACKS == 'True':\n",
    "    callbacks = [utils.tensorboard_callback(dir_name='training_logs', model_name=MODEL_NAME, exp_name=EXP_NAME),\n",
    "                 utils.early_stopping_callback,\n",
    "                 utils.reduce_lr_callback]\n",
    "elif CALLBACKS == 'False':\n",
    "    callbacks = [utils.tensorboard_callback(dir_name='training_logs', model_name=MODEL_NAME, exp_name=EXP_NAME)]\n",
    "\n",
    "# Fitting the model\n",
    "model.fit(train_ds,\n",
    "          epochs=NUM_EPOCHS,\n",
    "          steps_per_epoch=len(train_ds),\n",
    "          validation_data=test_ds,\n",
    "          validation_steps=len(test_ds),\n",
    "          callbacks=callbacks,\n",
    "          workers=NUM_WORKERS)\n",
    "\n",
    "# Save the model\n",
    "utils.save_model(model=model, dir_name='saved_model', model_name=MODEL_NAME, exp_name=EXP_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f78e75-d01e-488d-85fe-198356994565",
   "metadata": {},
   "source": [
    "## Prediction on saved model - predict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8906a32-1b37-4ee0-9272-f15bdca16b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing module/predict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile module/predict.py\n",
    "import os\n",
    "import cv2\n",
    "import argparse\n",
    "import numpy as np\n",
    "from sys import exit\n",
    "import tensorflow as tf\n",
    "\n",
    "# Creating a parser\n",
    "parser = argparse.ArgumentParser(description='Get some hyperparameters')\n",
    "\n",
    "# Getting hyperparameters\n",
    "# Sequence length\n",
    "parser.add_argument('--seq_len',\n",
    "                    default=20,\n",
    "                    type=int,\n",
    "                    help='Total number of frames for every video')\n",
    "\n",
    "# Frame size\n",
    "parser.add_argument('--frame_size',\n",
    "                    default=128,\n",
    "                    type=int,\n",
    "                    help='Integer for resizing the frame')\n",
    "\n",
    "# Video file path\n",
    "parser.add_argument('--video_path',\n",
    "                    type=str,\n",
    "                    help='File path of the video for predicting.')\n",
    "\n",
    "# Saved model\n",
    "parser.add_argument('--model_path',\n",
    "                    default='saved_model/lrcn_model2_2023-03-21-09:40:09_loss:0.1253_accuracy:0.9622',\n",
    "                    type=str,\n",
    "                    help='Target model path to use for the prediction.')\n",
    "\n",
    "# Class list\n",
    "parser.add_argument('--class_list',\n",
    "                    default=['Biking', 'Diving', 'GolfSwing', 'Punch', 'Rowing'],\n",
    "                    nargs='+',\n",
    "                    help='A list containing class names')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "SEQ_LEN = args.seq_len\n",
    "FRAME_SIZE = args.frame_size\n",
    "VIDEO_PATH = args.video_path\n",
    "MODEL_PATH = args.model_path\n",
    "CLASS_LIST = args.class_list\n",
    "\n",
    "print(f'[INFO] Predicting video file: \"{VIDEO_PATH}\" using model: \"{MODEL_PATH}\".')\n",
    "\n",
    "# Loading the model\n",
    "model = tf.keras.models.load_model(filepath=MODEL_PATH)\n",
    "print('[INFO] Model is been loaded and ready for prediction.')\n",
    "\n",
    "# function to process the video file\n",
    "def frames_extraction(video_file_path):\n",
    "    frames_list = []\n",
    "    \n",
    "    # Reading the video file and counting the frames\n",
    "    video_reader = cv2.VideoCapture(video_file_path)\n",
    "    video_frame_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Selecting the frames at certain interval and applying the transformation\n",
    "    skip_frames = max(int(video_frame_count/SEQ_LEN), 1)\n",
    "    for i in range(SEQ_LEN):\n",
    "        video_reader.set(cv2.CAP_PROP_POS_FRAMES, i * skip_frames)\n",
    "        success, frame = video_reader.read()\n",
    "        if not success:\n",
    "            break\n",
    "        resize_frame = cv2.resize(frame, (FRAME_SIZE, FRAME_SIZE))\n",
    "        norm_frame = resize_frame/255.\n",
    "        frames_list.append(norm_frame.astype('float32'))\n",
    "    video_reader.release()\n",
    "    return frames_list\n",
    "\n",
    "# Processing the video file\n",
    "frames_list = frames_extraction(video_file_path=VIDEO_PATH)\n",
    "print('[INFO] Video file is ready for prediction.')\n",
    "\n",
    "# predicting using the model \n",
    "pred_prob = model.predict(np.expand_dims(frames_list, axis=0))[0]\n",
    "pred_label = np.argmax(pred_prob)\n",
    "pred_class = args.class_list[pred_label]\n",
    "\n",
    "# printing the result\n",
    "print(f'[INFO] Action predicted by the model : {pred_class}')\n",
    "print(f'[INFO] Prediction probalities: {pred_prob[pred_label]:.2f}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
